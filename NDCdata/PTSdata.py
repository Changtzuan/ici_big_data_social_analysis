import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import os

# å»ºç«‹è³‡æ–™å¤¾ï¼ˆå¦‚å°šæœªå­˜åœ¨ï¼‰
os.makedirs(f"å³æ™‚æ–°èž/trump_articles_pts", exist_ok=True)

# å…¬è¦–æ–°èžæŠ“å–å‡½æ•¸
async def get_pts_article(url):
    from playwright.async_api import async_playwright, TimeoutError

    async with async_playwright() as p:
        browser = await p.firefox.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto(url, timeout=60000)
            await page.wait_for_selector("div.post-article")

            content_elements = await page.query_selector_all("div.post-article .articleimg, div.post-article p")
            content = []
            for el in content_elements:
                text = (await el.text_content() or '').strip()
                if text and text != '.':
                    content.append(text)

            if not content:
                print("âš ï¸ æ²’æœ‰æŠ“åˆ°ä»»ä½•å…§å®¹ï¼")
                return None
            return "\n".join(content)

        except TimeoutError:
            print("âš ï¸ é€£ç·šæˆ–è¼‰å…¥è¶…æ™‚ï¼Œç„¡æ³•æŠ“å–å…§å®¹ã€‚")
            return None
        except Exception as e:
            print(f"âš ï¸ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return None
        finally:
            await browser.close()


# ä¸»æµç¨‹
async def main():
    # è¼‰å…¥ CSV ä¸¦è½‰æ›æ™‚é–“
    df = pd.read_csv(r"å³æ™‚æ–°èž\trump_articles_breaking.csv", parse_dates=["ç™¼å¸ƒæ—¥æœŸ"])

    # éŽæ¿¾æ™‚é–“ç¯„åœ
    df = df[(df["ç™¼å¸ƒæ—¥æœŸ"] >= "2024-07-22") & (df["ç™¼å¸ƒæ—¥æœŸ"] <= "2024-11-07")]

    # ç¯©é¸å…¬è¦–æ–°èž
    pts_df = df[df["æ–°èžé€£çµ"].str.contains("pts.org.tw")].copy()

    # # ç¯©é¸æœ‰å‡ºç¾åœ¨è³‡æ–™å¤¾çš„ uid
    # folder_path = "TRUMPdata/å³æ™‚æ–°èž/NoData"
    # existing_files = set(os.listdir(folder_path))
    # # åŽ»æŽ‰å‰¯æª”å
    # existing_files = {file.split(".")[0] for file in existing_files}
    # # åªä¿ç•™åœ¨è³‡æ–™å¤¾ä¸­çš„ uid
    # pts_df = pts_df[pts_df["uid"].isin(existing_files)]
    # pts_df = pts_df.reset_index(drop=True)

    # æŠ“å–å…¨æ–‡
    print("ðŸ” æŠ“å–å…¬è¦–æ–°èžä¸­...")
    articles = []
    for i, url in enumerate(pts_df["æ–°èžé€£çµ"], 1):
        print(f"({i}/{len(pts_df)}) æŠ“å–ï¼š{url}")
        try:
            article = await get_pts_article(url)
        except Exception as e:
            article = f"éŒ¯èª¤ï¼š{str(e)}"
        articles.append(article)

    pts_df["fulltext"] = articles
    pts_df["fulltext"] = pts_df["fulltext"].astype(str)

    # ç¯©é¸åŒ…å«ã€Œå·æ™®ã€çš„æ–°èž
    df_trump = pts_df[
        pts_df["æ–°èžæ¨™é¡Œ"].str.contains("å·æ™®", na=False) |
        pts_df["æ–°èžå‰è¨€"].str.contains("å·æ™®", na=False) |
        pts_df["fulltext"].str.contains("å·æ™®", na=False)
    ]

    # å„²å­˜æ¯ç¯‡æ–‡ç« æˆ txt
    for _, row in df_trump.iterrows():
        file_path = f"å³æ™‚æ–°èž/trump_articles_pts/{row['uid']}.txt"
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(row["fulltext"])

    # å„²å­˜ CSVï¼ˆä¸å«ã€Œæ–°èžå…¨æ–‡ã€æ¬„ï¼‰
    df_trump.drop(columns=["fulltext"]).to_csv(f"å³æ™‚æ–°èž/trump_articles_pts.csv", index=False, encoding="utf-8-sig")
    print("âœ… å®Œæˆï¼šç¬¦åˆã€Žå·æ™®ã€çš„å…¬è¦–æ–°èžå·²å„²å­˜ã€‚")

# åŸ·è¡Œ
if __name__ == "__main__":
    asyncio.run(main())
