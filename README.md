# Trump's New World: Media Framing in Taiwan

A Comparative Study of the 2024 Election

## Project Context

This project explores how Taiwanese media framed Donald Trump during the 2024 U.S. presidential election by building a full-stack NLP pipeline for large-scale sentiment and framing analysis. We evaluated over 100,000 data and ultimately collected over 7,000 Mandarin-language news articles via authorized web crawling from sources such as UDN, PTS, Liberty Times, and ETtoday, all filtered using the keyword â€œå·æ™®â€ (Trump). After cleaning and structuring the data with dplyr and exporting to CSV, we used CKIPTagger for Mandarin-specific word segmentation, POS tagging, and named entity recognition.

For the sentiment evaluation, the core aim of the project was to benchmark the labeling and reasoning performance of leading large language models (LLMs) on Mandarin political news. We evaluated several LLMs such as ChatGPT-4o/4.1/o3, LLaMA 3.3, Qwen, and DeepSeek, using two classification frameworks: a single-stage direct sentiment classification, and a two-step reasoning-based labeling process with custom prompts. Accuracy was compared and calculated against human-labeled samples, revealing significant differences across models and prompt strategies.

Eventually, ChatGPT o3 demonstrated the most reliable and consistent performance. All visualizations and analytical results in this project are then based on its labeling output. Our analysis included chi-square tests, topic modeling, and correspondence analysis to reveal ideological patterns and framing biases across media outlets. This project illustrates how LLM-driven NLP workflows can be applied to multilingual media research, providing insight into political discourse and the computational challenges of sentiment modeling in non-English contexts.

## Getting Started

This project analyzes news sentiment regarding Trump using multiple data sources and AI models. Follow the instructions below to set up and run the complete analysis pipeline.


### **Data Sources**

Our research utilizes two primary data sources for comprehensive news analysis:

#### **ğŸ›ï¸ National Development Council (NDC) Open Data Portal**
- **Source Type**: Government authorized data
- **Data Volume**: ~6,000 articles (filtered from over 900,000 articles)
- **Content Filter**: All articles contain keyword "å·æ™®" (Trump)
- **Data Quality**: Official government data with high reliability
- **Access Method**: Web scraping through R scripts

#### **ğŸ“° UDN Knowledge Database** 
- **Source Type**: Commercial news media
- **Data Volume**: ~1,000 articles with full text
- **Content Filter**: All articles contain keyword "å·æ™®" (Trump)
- **Coverage**: Two major news sources
- **Access Method**: Web scraping through Python scripts

#### **ğŸ“Š Data Structure**
Each collected article contains:
- **Headlines and subheadlines**: Primary content identifiers
- **Full article content**: Complete text for analysis
- **News category/section**: Topic classification
- **Publication date**: Temporal analysis capability
- **Source media outlet**: Origin identification

#### **ğŸ—ï¸ Covered News Sources**
**UDN Group:**
- UDN (è¯åˆå ±)
- Economic Daily News (ç¶“æ¿Ÿæ—¥å ±)

**NDC Sources:**
- PTS News (å…¬è¦–æ–°è)
- Liberty Times (è‡ªç”±æ™‚å ±)
- ETtoday News (ETtoday æ–°è)
- Central News Agency (ä¸­å¤®ç¤¾)


### **AI Models Used**

This project evaluates multiple state-of-the-art language models for sentiment classification:

#### **ğŸ“‹ Model List**
- **OpenAI Models**: ChatGPT-o3, ChatGPT-o1, ChatGPT-o1-mini, ChatGPT-gpt-4o
- **DeepSeek Models**: DeepSeek R1 0528, DeepSeek V3 0324
- **Meta Models**: Llama 3.3 (70B), Llama 3.3 (19B)
- **NVIDIA Models**: Llama 3.1 (253B)
- **Google Models**: Gemma 3 (27B)
- **Microsoft Models**: Phi 4
- **Qwen Models**: Qwen3-235B-A22B, QwQ (32B)
- **Local Models**: Llama-3-Taiwan-70B

#### **ğŸ¯ Classification Approaches**

**Single-Stage Classification (Approach 1):**
```python
prompt = (
    "ä½ ç¾åœ¨æ˜¯æ–°èæƒ…ç·’åˆ†é¡å°ˆå®¶ï¼Œå°‡åˆ†é¡æ–°èå…§å®¹å°æ–¼å·æ™®æ˜¯æ”¯æŒã€ä¸­ç«‹ã€åå°ã€ç„¡é—œã€‚\n\n"
    "è«‹æŒ‰ç…§ä¸‹åˆ—æ¨™æº–é€²è¡Œæƒ…ç·’åˆ†é¡ï¼š\n\n"
    "ä¸­ç«‹ï¼šå°æ–¼å·æ™®çš„ç›¸é—œå…§å®¹ï¼Œå®¢è§€é™³è¿°äº‹å¯¦ï¼Œæ–‡å­—å¹³é‹ªç›´æ•˜ï¼Œä¸å¸¶æƒ…ç·’è‰²å½©ã€‚\n\n"
    "æ”¯æŒï¼šå°æ–¼å·æ™®çš„ç›¸é—œå…§å®¹ï¼Œæ­£é¢å‚³é”å°å·æ™®æœ‰åˆ©çš„è¨Šæ¯ï¼Œæˆ–è®“è®€è€…å°å·æ™®ç”¢ç”Ÿå¥½å°è±¡ã€‚ä¾‹å¦‚ï¼šå¼·èª¿å…¶æ”¿ç¸¾ã€é ˜è¢–ç‰¹è³ªã€æ­£é¢è©å½™ã€æ”¯æŒè€…çš„è²éŸ³ï¼Œæˆ–æ˜é¡¯è²¶ä½å…¶å°æ‰‹ã€‚\n\n"
    "åå°ï¼šå°æ–¼å·æ™®çš„ç›¸é—œå…§å®¹ï¼Œè² é¢å‚³é”å°å·æ™®ä¸åˆ©çš„è¨Šæ¯ï¼Œæˆ–è®“è®€è€…å°å·æ™®ç”¢ç”Ÿè² é¢å°è±¡ã€‚ä¾‹å¦‚ï¼šå¼·èª¿çˆ­è­°ã€è² é¢äº‹ä»¶ã€æ‰¹è©•æ€§æªè¾­ã€å¼•ç”¨åå°è€…è§€é»å±…å¤šã€‚\n\n"
    "ç„¡é—œï¼šå¦‚æœå…§å®¹èˆ‡å·æ™®å®Œå…¨ç„¡é—œã€‚\n\n"
    "è«‹ä»”ç´°é–±è®€ä»¥ä¸‹æ–°èæ¨™é¡ŒåŠå…§å®¹ï¼Œè©•ä¼°é€™ç¯‡æ–°èå°å·æ™®çš„ç«‹å ´ï¼Œè«‹ä¸è¦å›å‚³å…¶ä»–æ–‡å­—æˆ–æ¨™é»ç¬¦è™Ÿï¼Œåªå›å‚³æ”¯æŒã€ä¸­ç«‹ã€åå°ã€ç„¡é—œï¼š\n\n"
    f"æ–°èæ¨™é¡Œï¼š{title}\n"
    f"æ–°èå‡ºè‡ªï¼š{media}\n"
    f"æ–°èå…¨æ–‡ï¼š{news_content}"
)
```

**Two-Stage Reasoning Classification (Approach 2):**

*Step 1: Emotion Sentence Extraction*
```python
prompt_step1 = (
    "ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼æ–°èæƒ…æ„Ÿåˆ†æçš„AIã€‚ä½ çš„ä»»å‹™æ˜¯ä»”ç´°é–±è®€ä»¥ä¸‹æ–°èå…§å®¹ï¼Œä¸¦å°ˆæ³¨æ–¼æ‰¾å‡ºä»»ä½•å°ä¸»è¦å¯¦é«”ã€Œå·æ™®ã€å¸¶æœ‰æƒ…æ„Ÿè‰²å½©çš„æè¿°ã€‚\n\n"
    "è«‹éµå¾ªä»¥ä¸‹æŒ‡ç¤ºï¼š\n\n"
    "1.  **è­˜åˆ¥æƒ…æ„Ÿå¥ï¼š** å¾æ–°èæ–‡æœ¬ä¸­ï¼Œé€å¥æå–æ‰€æœ‰ç›´æ¥æè¿°ã€Œå·æ™®ã€ä¸¦å¸¶æœ‰æ˜é¡¯æ­£é¢æˆ–è² é¢æƒ…æ„Ÿï¼ˆä¾‹å¦‚ï¼šè®šæšã€æ‰¹è©•ã€å–œæ„›ã€å­æƒ¡ã€å˜²è«·ã€åŒæƒ…ç­‰ï¼‰çš„å¥å­ã€‚è«‹åˆ—å‡ºé€™äº›å¥å­çš„åŸæ–‡ã€‚\n"
    "2.  **ä¸­ç«‹å…§å®¹è™•ç†ï¼š** å¦‚æœæ–°èå…§å®¹åƒ…ç‚ºå®¢è§€äº‹å¯¦é™³è¿°ï¼Œç”¨è©ä¸­æ€§ï¼Œæœªå°ã€Œå·æ™®ã€è¡¨é”ä»»ä½•æƒ…æ„Ÿåå‘ï¼Œä¸”å¯èƒ½å¹³è¡¡å‘ˆç¾äº†ä¸åŒè§€é»ï¼Œè«‹ç›´æ¥å›è¦†ã€Œæœ¬æ–°èå…§å®¹ç‚ºä¸­ç«‹ã€‚ã€ï¼Œç„¡éœ€åˆ—èˆ‰å¥å­ã€‚\n"
    "3.  **ç„¡é—œå…§å®¹è™•ç†ï¼š** å¦‚æœæ–°èå…§å®¹å®Œå…¨æœªæåŠã€Œå·æ™®ã€ï¼Œæˆ–è€…åƒ…åœ¨èˆ‡æ–°èä¸»é¡Œç„¡é—œçš„èƒŒæ™¯è³‡è¨Šä¸­æ¥µå…¶ç°¡ç•¥åœ°æåŠï¼Œä¸”è©²æåŠä¸å¸¶ä»»ä½•æƒ…æ„Ÿè‰²å½©ï¼Œè«‹ç›´æ¥å›è¦†ã€Œæœ¬æ–°èå…§å®¹ç‚ºç„¡é—œã€‚ã€ï¼Œç„¡éœ€åˆ—èˆ‰å¥å­ã€‚\n"
    "4.  **è¼¸å‡ºæ ¼å¼ï¼š**\n"
    "    *   è‹¥ç‚ºæƒ…æ„Ÿå¥ï¼Œè«‹ç›´æ¥åˆ—å‡ºåŸæ–‡ï¼Œæ¯å¥ä¸€è¡Œã€‚ä¸è¦æ·»åŠ ä»»ä½•é¡å¤–çš„è§£é‡‹ã€ç·¨è™Ÿæˆ–è©•è«–ã€‚\n"
    "    *   è‹¥ç‚ºä¸­ç«‹æˆ–ç„¡é—œï¼Œå‰‡æŒ‰ä¸Šè¿°æŒ‡ç¤ºå›è¦†ç‰¹å®šçŸ­èªã€‚\n\n"
    "è«‹é–‹å§‹åˆ†æä»¥ä¸‹æ–°èæ¨™é¡Œèˆ‡å…§å®¹ï¼š"
)

completion = client.chat.completions.create(
    model="o3",  # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
    messages=[{"role": "system", "content": prompt_step1},
              {"role": "user", "content": f"{title}\n\n{news_content}"}],
    service_tier="flex"
)
```

*Step 2: Sentiment Classification*
```python
prompt_step2 = (
    "ä½ æ˜¯ä¸€ä½æƒ…æ„Ÿåˆ†æå°ˆå®¶ã€‚ä»¥ä¸‹æ–‡å­—æ˜¯å¾ä¸€ç¯‡é—œæ–¼ã€Œå·æ™®ã€çš„æ–°èä¸­æå–å‡ºçš„å¸¶æœ‰æƒ…æ„Ÿè‰²å½©çš„å¥å­ã€‚è«‹åŸºæ–¼é€™äº›å¥å­ï¼Œåˆ¤æ–·æ–°èå…§å®¹å°ã€Œå·æ™®ã€çš„æ•´é«”æƒ…æ„Ÿç«‹å ´ã€‚\n\n"
    "è«‹éµå¾ªä»¥ä¸‹æŒ‡ç¤ºï¼š\n\n"
    "1.  **åˆ¤æ–·ç«‹å ´ï¼š** ç¶œåˆåˆ†ææä¾›çš„æ‰€æœ‰å¥å­ï¼Œåˆ¤æ–·æ•´é«”æƒ…æ„Ÿæ˜¯ã€Œæ”¯æŒã€å·æ™®é‚„æ˜¯ã€Œåå°ã€å·æ™®ã€‚\n"
    "2.  **ç°¡æ½”å›è¦†ï¼š** ä½ çš„å›ç­”å¿…é ˆä¸”åªèƒ½æ˜¯ã€Œæ”¯æŒã€æˆ–ã€Œåå°ã€é€™å…©å€‹è©ä¸­çš„ä¸€å€‹ã€‚ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€è§£é‡‹ã€æ¨™é»ç¬¦è™Ÿæˆ–ç©ºæ ¼ã€‚\n\n"
    "è«‹åˆ†æä»¥ä¸‹å…§å®¹ä¸¦çµ¦å‡ºä½ çš„åˆ¤æ–·ï¼š"
)

senti_completion = client.chat.completions.create(
    model="o3",  # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
    messages=[{"role": "system", "content": prompt_step2}, 
              {"role": "user", "content": response_content}],
    service_tier="flex"
)
```

### Project Workflow

#### Workflow Diagram

```mermaid
flowchart LR
    subgraph DC ["ğŸ“Š Data Collection Phase"]
      direction TB
      subgraph NEWS ["News Sources"]
        direction TB
        A1["ğŸ›ï¸ NDC News Collection<br/>(Various Category R Scripts)"] 
        A2["ğŸ“º PTS News Collection<br/>(PTSdata.py)"]
        A3["ğŸ“° UDN News Collection<br/>(UDNdata.py)"]
      end
      
      A1 --> B1["ğŸ“„ NDC Articles<br/>ndc_articles.csv"]
      A2 -.-> B2["ğŸ“„ PTS Articles<br/>pts_articles.csv"]
      A3 --> B3["ğŸ“„ UDN Articles<br/>udn_articles.csv"]
      
      B2 -.->|Manual Merge| B1
    end
    
    %% Direct connections from Data Collection to branches
    DC --> TP_DIST[ ]
    DC --> SB_DIST[ ]
    DC --> DI_DIST[ ]
    
    subgraph TP ["ğŸ” Text Processing Branch"]
      direction TB
      TP_DIST -.-> D1["âš™ï¸ CKIP_NDC.R<br/>Chinese NLP Processing"]
      TP_DIST -.-> D2["âš™ï¸ CKIP_UDN.R<br/>Chinese NLP Processing"]
      
      D1 --> E1["ğŸ“‹ NDC Processed<br/>â€¢ ndc_articles_POS.csv<br/>â€¢ ndc_articles_NER.csv"]
      D2 --> E2["ğŸ“‹ UDN Processed<br/>â€¢ udn_articles_POS.csv<br/>â€¢ udn_articles_NER.csv"]
    end
    
    subgraph SB ["ğŸ¯ Sampling Branch"]
      direction TB
      SB_DIST -.-> F1["ğŸ² Sample.py<br/>Random Selection<br/>(NDC + UDN Articles)"]
      F1 --> G1["ğŸ“ 100 Sampled Articles<br/>â€¢ ndc_articles_sampled.csv<br/>â€¢ udn_articles_sampled.csv"]
      G1 --> H2["ğŸ”§ MergeData.R<br/>(Sampling)"]
      H2 --> I2["ğŸ“Š Sampled Dataset<br/>sampled_articles.csv"]
    end
    
    subgraph DI ["ğŸ”— Data Integration Branch"]
      direction TB
      DI_DIST -.-> H1["ğŸ”§ MergeData.R<br/>(Complete Dataset)<br/>(NDC + UDN Articles)"]
      H1 --> I1["ğŸ“Š Complete Dataset<br/>all_articles.csv"]
    end
    
    %% Manual Analysis entry
    I2 --> MA_DIST[ ]
    
    subgraph MA ["ğŸ‘¥ Manual Analysis Phase"]
      direction TB
      MA_DIST -.-> J1["âœï¸ Manual Labeling<br/>Ground Truth Creation"]
      J1 --> K1["ğŸ“‹ Labelled Dataset<br/>Labelled.csv"]
    end
    
    %% AI Analysis entries
    I1 --> AI_DIST1[ ]
    K1 --> AI_DIST2[ ]
    
    subgraph AI ["ğŸ¤– AI Analysis Phase"]
      direction TB
      AI_DIST1 -.-> L1["ğŸš€ AI Analysis<br/>(Complete Dataset)"]
      AI_DIST2 -.-> L2["ğŸš€ AI Analysis<br/>(Labeled Dataset)"]
      
      L1 --> M1["ğŸ”§ Label_OneStep.py<br/>Label_TwoSteps.py"]
      L2 --> M2["ğŸ”§ Label_OneStep.py<br/>Label_TwoSteps.py"]
      
      M1 --> N1["ğŸ“ˆ AI Results<br/>all_articles_results.csv"]
      M2 --> N2["ğŸ“Š LLMsSCORE<br/>Model Comparison Results"]
    end
    
    %% Final Analysis entries
    E1 --> FA_DIST1[ ]
    E2 --> FA_DIST2[ ]
    N1 --> FA_DIST3[ ]
    N2 --> FA_DIST4[ ]
    
    subgraph FA ["ğŸ“ˆ Final Analysis Phase"]
      direction TB
      FA_DIST1 -.-> O1["ğŸ”¬ Comprehensive Analysis"]
      FA_DIST2 -.-> O1
      FA_DIST3 -.-> O1
      FA_DIST4 -.-> O1
      O1 --> P1["ğŸ“‹ Final Research Results<br/>Combined Insights"]
    end
    
    %% Styling for subgraphs
    style DC fill:#e1f5fe,stroke:#01579b,stroke-width:4px,color:#000
    style NEWS fill:#f0f8ff,stroke:#4682b4,stroke-width:2px,color:#000
    style TP fill:#f3e5f5,stroke:#4a148c,stroke-width:4px,color:#000
    style SB fill:#fff3e0,stroke:#e65100,stroke-width:4px,color:#000
    style DI fill:#e8f5e8,stroke:#1b5e20,stroke-width:4px,color:#000
    style MA fill:#fce4ec,stroke:#880e4f,stroke-width:4px,color:#000
    style AI fill:#fff8e1,stroke:#f57f17,stroke-width:4px,color:#000
    style FA fill:#f1f8e9,stroke:#33691e,stroke-width:4px,color:#000
    
    %% Styling for nodes
    classDef collection fill:#e1f5fe,stroke:#01579b,stroke-width:3px,color:#000,font-size:14px
    classDef processing fill:#f3e5f5,stroke:#4a148c,stroke-width:3px,color:#000,font-size:14px
    classDef sampling fill:#fff3e0,stroke:#e65100,stroke-width:3px,color:#000,font-size:14px
    classDef integration fill:#e8f5e8,stroke:#1b5e20,stroke-width:3px,color:#000,font-size:14px
    classDef manual fill:#fce4ec,stroke:#880e4f,stroke-width:3px,color:#000,font-size:14px
    classDef ai fill:#fff8e1,stroke:#f57f17,stroke-width:3px,color:#000,font-size:14px
    classDef final fill:#f1f8e9,stroke:#33691e,stroke-width:4px,color:#000,font-size:14px
    classDef data fill:#f5f5f5,stroke:#424242,stroke-width:2px,color:#000,font-size:12px
    classDef invisible fill:transparent,stroke:transparent,color:transparent
    
    class A1,A2,A3 collection
    class D1,D2 processing
    class F1 sampling
    class H1,H2 integration
    class J1 manual
    class L1,L2,M1,M2 ai
    class O1 final
    class B1,B2,B3,E1,E2,G1,I1,I2,K1,N1,N2,P1 data
    class TP_DIST,SB_DIST,DI_DIST,MA_DIST,AI_DIST1,AI_DIST2,FA_DIST1,FA_DIST2,FA_DIST3,FA_DIST4 invisible
```

#### Legend
- ğŸ“Š **Data Collection**: Gathering news articles from multiple sources
- ğŸ” **Text Processing**: NLP processing using CKIP tools
- ğŸ¯ **Sampling**: Random selection for manual analysis
- ğŸ”— **Data Integration**: Combining all datasets
- ğŸ‘¥ **Manual Analysis**: Human labeling for ground truth
- ğŸ¤– **AI Analysis**: Automated labeling and comparison
- ğŸ“ˆ **Final Analysis**: Comprehensive results synthesis


### **Installation Requirements**

#### **Prerequisites**
- **R**: Version 4.0+ recommended
- **Python**: Version 3.8+ recommended
- **OpenAI API Key**: Required for AI analysis
- **Internet Connection**: Required for data collection and API calls

#### **R Environment**
Make sure you have R installed. Install the required R packages by running the following commands in your R console:

```R
install.packages("readr")       # For reading and writing CSV files
install.packages("rvest")       # For web scraping
install.packages("dplyr")       # For data manipulation
install.packages("stringr")     # For string processing
install.packages("purrr")       # For functional programming
install.packages("httr")        # For HTTP requests
install.packages("progressr")   # For progress bar display
install.packages("reticulate")  # To call Python from R
```

#### **Python Environment**
Ensure you have Python installed (recommended version: 3.8+). Install the required Python packages using `pip`:

```plaintext
pip install openai==1.78.1       # Interaction with OpenAI API
pip install pandas==2.0.3        # Data manipulation
pip install playwright==1.48.0   # Asynchronous browser automation
pip install selenium==4.10.0     # Browser automation
pip install ckiptagger==0.2.1    # Chinese word segmentation
pip install tqdm==4.67.1         # Progress bar display
```

#### **CKIPTagger Setup**
CKIPTagger is essential for Chinese NLP processing. Follow these steps:

1. **Installation**: Follow the official guide at [https://github.com/ckiplab/ckiptagger](https://github.com/ckiplab/ckiptagger)
2. **Model Download**: Download the required pre-trained models as specified in their documentation
3. **Verification**: Test the installation before running the main scripts

#### **API Configuration**
- **OpenAI API**: 
   - Obtain an API key from OpenAI
   - Set the API key in your environment variables
   - Configure rate limits to avoid API quota issues

```python
import openai
client = openai.OpenAI(api_key="your-api-key-here")
```


### **Execution Flow Summary**

```
Data Collection â†’ [Text Processing | Sampling | Data Integration] 
                     â†“              â†“            â†“
                Text Results   Manual Analysis  Complete Dataset
                     â†“              â†“            â†“
                     â””â”€â”€â”€â”€ AI Analysis Phase â”€â”€â”€â”€â”˜
                              â†“
                        Final Analysis
```


### **Important Notes**

- **Parallel Execution**: Phases 2 (Branches A, B, C) can be run in parallel after Phase 1 completion
- **Dependencies**: 
  - Phase 3 (Manual Analysis) requires Branch B (Sampling) completion
  - Phase 4 (AI Analysis) requires Branches B and C completion
  - Phase 5 (Final Analysis) requires all previous phases
- **CKIPTagger Setup**: Ensure CKIPTagger models are properly installed before running text processing scripts
- **API Configuration**: Configure OpenAI API keys before running AI analysis scripts
- **File Management**: Ensure output directories exist and have proper write permissions

### **Expected Outputs**

After completing all steps, you should have:

- **ğŸ“Š Raw Data**: `ndc_articles.csv`, `udn_articles.csv`, `pts_articles.csv`
- **ğŸ” Processed Data**: `*_POS.csv`, `*_NER.csv` files with NLP annotations
- **ğŸ¯ Sample Data**: `sampled_articles.csv` with 100 selected articles
- **ğŸ‘¥ Ground Truth**: `Labelled.csv` with manual sentiment labels
- **ğŸ¤– AI Results**: Model performance comparisons and sentiment predictions
- **ğŸ“ˆ Final Analysis**: Comprehensive research insights and findings

### **Troubleshooting**

#### **Common Issues**
- **Memory Errors**: Process large datasets in smaller batches
- **API Rate Limits**: Implement delays between API calls
- **Encoding Issues**: Ensure UTF-8 encoding for Chinese text
- **Missing Dependencies**: Verify all packages are properly installed
- **Network Timeouts**: Check internet connection for data collection

#### **Support Resources**
- **CKIPTagger Issues**: Refer to their GitHub repository
- **OpenAI API Problems**: Check OpenAI documentation
- **R Package Issues**: Use `install.packages()` with dependencies=TRUE
- **Python Environment**: Consider using virtual environments

## File Structure

### **Project Directory Structure**

```plaintext
ici_big_data_social_analysis\                  # Project root directory
|
â”œâ”€â”€ .git\                                      # Git version control folder
|
â”œâ”€â”€ LLMsSCORE\                                 # Scores of various LLMs for classifying 100 sampled articles using two prompts
â”‚   â”œâ”€â”€ [LLM result-related files]
â”‚
â”œâ”€â”€ NDCdata\                                   # NDC news-related data
â”‚   â”œâ”€â”€ ndc_articles_sampled\                  # Stores full text of 100 randomly sampled articles
â”‚   â”œâ”€â”€ trump_articles\                        # Stores full text of all categorized news articles
â”‚   â”œâ”€â”€ trump_articles_POS_TXT\                # Stores WS+POS processed full text of all categorized news articles
â”‚   â”œâ”€â”€ [News category folders]                # E.g., Cross-Strait News, Breaking News, etc.
â”‚   â”‚   â”œâ”€â”€ trump_articles\                    # Full text of news articles in the specific category
â”‚   â”‚   â”œâ”€â”€ trump_articles_[category].R        # Script for scraping and processing news articles in the category
â”‚   â”‚   â”œâ”€â”€ trump_articles_[category].csv      # Exported news data for the category (output from trump_articles_[category].R)
â”‚   â”œâ”€â”€ ndc_articles.csv                       # Complete dataset of NDC news (output from CKIP_NDC.R)
â”‚   â”œâ”€â”€ ndc_articles_NER.csv                   # NER results for NDC news (output from CKIP_NDC.R)
â”‚   â”œâ”€â”€ ndc_articles_POS.csv                   # WS+POS results for NDC news (output from CKIP_NDC.R)
â”‚   â”œâ”€â”€ ndc_articles_sampled.csv               # 100 randomly sampled NDC news articles (output from Sample.py)
â”‚   â”œâ”€â”€ PTSdata.py                             # Script using Playwright to scrape NDC news
â”‚
â”œâ”€â”€ UDNdata\                                   # UDN news-related data
â”‚   â”œâ”€â”€ trump_articles\                        # Stores full text of all categorized news articles
â”‚   â”œâ”€â”€ trump_articles_POS_TXT\                # Stores WS+POS processed full text of all categorized news articles
â”‚   â”œâ”€â”€ udn_articles_sampled\                  # Stores full text of 100 randomly sampled articles
â”‚   â”œâ”€â”€ udn_articles.csv                       # Complete dataset of UDN news (output from UDNdata.py)
â”‚   â”œâ”€â”€ udn_articles_NER.csv                   # NER results for UDN news (output from CKIP_UDN.R)
â”‚   â”œâ”€â”€ udn_articles_POS.csv                   # WS+POS results for UDN news (output from CKIP_UDN.R)
â”‚   â”œâ”€â”€ udn_articles_sampled.csv               # 100 randomly sampled UDN news articles (output from Sample.py)
â”‚   â”œâ”€â”€ UDNdata.py                             # Script using Selenium to scrape UDN news
â”‚
â”œâ”€â”€ sample_articles\                           # Stores full text of 100 randomly sampled articles (combined from NDC and UDN)
â”‚
â”œâ”€â”€ all_articles.csv                           # Complete dataset of all news articles (output from MergeData.R)
â”œâ”€â”€ all_articles_results.csv                   # AI model analysis results for all news articles (using OpenAI-o3, output from Label_OneStep.py)
â”œâ”€â”€ sampled_articles.csv                       # 100 randomly sampled articles (output from MergeData.R)
â”œâ”€â”€ Labelled.csv                               # 100 manually labeled news articles
|
â”œâ”€â”€ CKIP_NDC.R                                 # Uses CKIPTagger to analyze NDC news data
â”œâ”€â”€ CKIP_UDN.R                                 # Uses CKIPTagger to analyze UDN news data
â”œâ”€â”€ MergeData.R                                # Combines and processes multiple datasets
|
â”œâ”€â”€ Label_OneStep.py                           # Uses OpenAI API for one-step sentiment and label analysis of news articles
â”œâ”€â”€ Label_TwoSteps.py                          # Uses OpenAI API for two-step sentiment and label analysis of news articles
â”œâ”€â”€ Sample.py                                  # Randomly selects 100 news articles for manual labeling
```

### **File Relationships**

| **Output File**             | **Source Code**              | **Description**                                                                 |
|-----------------------------|-----------------------------|-------------------------------------------------------------------------------|
| `trump_articles_[category].csv` | `trump_articles_[category].R` | Exported news data for each category, including the full text of news articles |
| `ndc_articles.csv`          | `CKIP_NDC.R`               | Complete dataset of NDC news                                                 |
| `ndc_articles_NER.csv`      | `CKIP_NDC.R`               | NER results for NDC news                                                     |
| `ndc_articles_POS.csv`      | `CKIP_NDC.R`               | WS+POS results for NDC news                                                  |
| `ndc_articles_sampled.csv`  | `Sample.py`                | 100 randomly sampled NDC news articles                                       |
| `udn_articles.csv`          | `UDNdata.py`              | Complete dataset of UDN news                                                 |
| `udn_articles_NER.csv`      | `CKIP_UDN.R`              | NER results for UDN news                                                     |
| `udn_articles_POS.csv`      | `CKIP_UDN.R`              | WS+POS results for UDN news                                                  |
| `udn_articles_sampled.csv`  | `Sample.py`                | 100 randomly sampled UDN news articles                                       |
| `sampled_articles.csv`      | `MergeData.R`             | Combined 100 randomly sampled articles from NDC and UDN                      |
| `all_articles.csv`          | `MergeData.R`             | Combined dataset of all NDC and UDN news articles                            |
| `all_articles_results.csv`  | `Label_OneStep.py`        | Sentiment and label analysis results for all news articles using OpenAI-o3   |
| `Labelled.csv`              | Manual labeling           | 100 manually labeled news articles                                           |


## Analysis

### Analysis Methods & Visualizations
We applied a full-stack NLP pipeline and multiple visualization techniques to uncover how Taiwanese media framed Donald Trump during the 2024 U.S. presidential election. Below are the core analyses and their associated insights:

### ğŸ§  Vocabulary & Framing Patterns
![Q1_ Top 20 Trump-Related Words (Media Composition)](https://github.com/user-attachments/assets/2cf27ff1-8fb6-4772-a875-26af842ed947)

We analyzed word usage across outlets using CKIPTagger-based segmentation. Common patterns included high-frequency personal names (Trump, Biden, Harris) and evaluative terms like believe, may, and state, indicating personalized and subjective media framing.

### ğŸ“° Sentiment by Media Outlet
åœ–

*We compared sentiment polarity (Supportive / Neutral / Oppositional) using both CSentiPackage and LLM-based labeling. PTS and Liberty Times showed more positive framing, while CNA and ETtoday remained mostly neutralâ€”reflecting ideological variance across the media landscape.*

### ğŸ” Statistical Significance of Framing
åœ–

*A chi-square test revealed statistically significant framing deviations. For instance, PTS had more positive coverage than expected, while ETtoday significantly underrepresented such framingâ€”highlighting bias patterns aligned with outlet orientation.*

### ğŸ•’ Temporal Coverage Trends
åœ–

*Media attention followed a U-shaped curve: peaking after Bidenâ€™s withdrawal (July) and Trumpâ€™s victory (November), with a lull mid-campaign. The sharp rise in late October aligned with election momentum and reflected media re-engagement.*

### ğŸ“ˆ Sentiment Shifts Over Time
åœ–

*Neutral reporting dominated (>75%) throughout the cycle. However, positive sentiment peaked after Trumpâ€™s election win, suggesting media shifted tone in response to political outcomes. Negative coverage remained minimal.*

### ğŸ§¾ Entity Network Analysis
åœ–

*Using NER, we observed that PERSON entities (Trump, Harris, etc.) dominated, followed by geopolitical (GPE) and organizational (ORG) terms. This reflects both the personalization of news and its anchoring in global political context.*

### ğŸ¤– LLM Performance Benchmark
![InnoFest (1)](https://github.com/user-attachments/assets/c06e6ebe-68a6-45e2-a243-b1a56051416a)

![InnoFest (4)](https://github.com/user-attachments/assets/d07fe31a-63e0-47f3-84b4-609ee8eda014)

We evaluated multiple LLMs (ChatGPT-4o/4.1, DeepSeek, Qwen, LLaMA 3.3, etc.) in both single-stage and multi-stage sentiment classification. ChatGPT and DeepSeek performed best in one-step reasoning, but complex prompts exposed limitations in model reliability and precision across tasks.
## Results

[Provide a summary of your findings and conclusions, including any recommendations or implications for future research. Be sure to explain how your results address your research question or problem statement.]

## Contributors

[List the contributors to your project and describe their roles and responsibilities.]

## Acknowledgments

[Thank any individuals or organizations who provided support or assistance during your project, including funding sources or data providers.]

## References

### Data Sources
UDN Knowledge Database (è¯åˆå ±è³‡æ–™åº«) â€“ via National Development Council Open Data Portal

News articles from:

United Daily News (UDN) / PTS News Network (å…¬è¦–æ–°è) / Liberty Times (è‡ªç”±æ™‚å ±) / Economic Daily News (ç¶“æ¿Ÿæ—¥å ±) / ETtoday News / Central News Agency (CNA)

### Analytical Tools & Methods
Web Crawling & Data Cleaning: 
R with dplyr, CSV conversion

Text Preprocessing: 
CKIPTagger for Mandarin NLP / Word Segmentation (WS) / Part-of-Speech Tagging (POS) / Named Entity Recognition (NER)

Sentiment Analysis Tools:
Custom prompt engineering with multiple LLMs:
ChatGPT (4o, 4.1) / Meta LLaMA 3.3 / DeepSeek R1 / Qwen-3 / Microsoft Phi-4 / Google Gemma 3

### Statistical & Text Analysis
Chi-Square Test for independence / Standardized Residuals (Z-scores) / Topic Modeling / Correspondence Analysis
